@article{radford2021learning,
  title={Learning Transferable Visual Models From Natural Language Supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and others},
  journal={arXiv preprint arXiv:2103.00020},
  year={2021}
}

@misc{rombach2021highresolution,
      title={High-Resolution Image Synthesis with Latent Diffusion Models}, 
      author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},
      year={2021},
      eprint={2112.10752},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{ruiz2023dreambooth,
      title={DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation}, 
      author={Nataniel Ruiz and Yuanzhen Li and Varun Jampani and Yael Pritch and Michael Rubinstein and Kfir Aberman},
      year={2023},
      eprint={2208.12242},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{kokalj-etal-2021-bert,
    title = "{BERT} meets Shapley: Extending {SHAP} Explanations to Transformer-based Classifiers",
    author = "Kokalj, Enja  and
      {\v{S}}krlj, Bla{\v{z}}  and
      Lavra{\v{c}}, Nada  and
      Pollak, Senja  and
      Robnik-{\v{S}}ikonja, Marko",
    booktitle = "Proceedings of the EACL Hackashop on News Media Content Analysis and Automated Report Generation",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.hackashop-1.3",
    pages = "16--21",
    abstract = "Transformer-based neural networks offer very good classification performance across a wide range of domains, but do not provide explanations of their predictions. While several explanation methods, including SHAP, address the problem of interpreting deep learning models, they are not adapted to operate on state-of-the-art transformer-based neural networks such as BERT. Another shortcoming of these methods is that their visualization of explanations in the form of lists of most relevant words does not take into account the sequential and structurally dependent nature of text. This paper proposes the TransSHAP method that adapts SHAP to transformer models including BERT-based text classifiers. It advances SHAP visualizations by showing explanations in a sequential manner, assessed by human evaluators as competitive to state-of-the-art solutions.",
}

@misc{vaswani2023attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@article{shapley1953value,
  title={A value for n-person games},
  author={Shapley, Lloyd S},
  journal={Contributions to the Theory of Games},
  volume={2},
  number={28},
  pages={307--317},
  year={1953},
  publisher={Princeton University Press}
}

@misc{ghorbani2019data,
      title={Data Shapley: Equitable Valuation of Data for Machine Learning}, 
      author={Amirata Ghorbani and James Zou},
      year={2019},
      eprint={1904.02868},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{lundberg2017unified,
      title={A Unified Approach to Interpreting Model Predictions}, 
      author={Scott Lundberg and Su-In Lee},
      year={2017},
      eprint={1705.07874},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{carlini2023extracting,
      title={Extracting Training Data from Diffusion Models}, 
      author={Nicholas Carlini and Jamie Hayes and Milad Nasr and Matthew Jagielski and Vikash Sehwag and Florian Tramèr and Borja Balle and Daphne Ippolito and Eric Wallace},
      year={2023},
      eprint={2301.13188},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@misc{image-style-study-2023-part1,
    title = {Image Synthesis Style Studies Database (The List)},
    author={@sureailabs and @proximasan and @EErratica and @KyrickYoung},
    howpublished = {\url{https://docs.google.com/spreadsheets/d/14xTqtuV3BuKDNhLotB_d1aFlBGnDJOY0BRXJ8-86GpA/edit?usp=sharing}},
    note = {Accessed: 2023-10-02},
}

@misc{pinkney2022imagemixer,
      title={ImageMixer: A Multi-Image Conditioning GAN for Image Synthesis}, 
      author={Justin Pinkney},
      year={2022},
      eprint={2206.05524},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{ramesh2021zeroshot,
      title={Zero-Shot Text-to-Image Generation}, 
      author={Aditya Ramesh and Mikhail Pavlov and Gabriel Goh and Scott Gray and Chelsea Voss and Alec Radford and Mark Chen and Ilya Sutskever},
      year={2021},
      eprint={2102.12092},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{brown2020language,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{razavi2019generating,
      title={Generating Diverse High-Fidelity Images with VQ-VAE-2}, 
      author={Ali Razavi and Aaron van den Oord and Oriol Vinyals},
      year={2019},
      eprint={1906.00446},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{crowson2021clip,
    author = {Crowson, Katherine},
    title = {{CLIP guided diffusion HQ 256x256}},
    year = {2021},
    howpublished = {\url{https://colab.research.google.com/drive/12a_Wrfi2_gwwAuN3VvMTwVMz9TfqctNj}}
}

@misc{crowson2021clip512,
    author = {Crowson, Katherine},
    title = {{CLIP Guided Diffusion 512x512, Secondary Model Method}},
    year = {2021},
    howpublished = {\url{https://twitter.com/RiversHaveWings/status/1462859669454536711}}
}

@misc{nichol2021improved,
      title={Improved Denoising Diffusion Probabilistic Models}, 
      author={Alex Nichol and Prafulla Dhariwal},
      year={2021},
      eprint={2102.09672},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{nichol2022glide,
      title={GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models}, 
      author={Alex Nichol and Prafulla Dhariwal and Aditya Ramesh and Pranav Shyam and Pamela Mishkin and Bob McGrew and Ilya Sutskever and Mark Chen},
      year={2022},
      eprint={2112.10741},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{alembics2022disco,
    author = {Alembics},
    title = {Disco Diffusion},
    year = {2022},
    howpublished = {\url{https://github.com/alembics/disco-diffusion}},
    note = {[Online; accessed 26-11-2023]}
}

@misc{ramesh2022hierarchical,
      title={Hierarchical Text-Conditional Image Generation with CLIP Latents}, 
      author={Aditya Ramesh and Prafulla Dhariwal and Alex Nichol and Casey Chu and Mark Chen},
      year={2022},
      eprint={2204.06125},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{saharia2022photorealistic,
      title={Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding}, 
      author={Chitwan Saharia and William Chan and Saurabh Saxena and Lala Li and Jay Whang and Emily Denton and Seyed Kamyar Seyed Ghasemipour and Burcu Karagol Ayan and S. Sara Mahdavi and Rapha Gontijo Lopes and Tim Salimans and Jonathan Ho and David J Fleet and Mohammad Norouzi},
      year={2022},
      eprint={2205.11487},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{raffel2023exploring,
      title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}, 
      author={Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
      year={2023},
      eprint={1910.10683},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{cherti2022reproducible,
      title={Reproducible scaling laws for contrastive language-image learning}, 
      author={Mehdi Cherti and Romain Beaumont and Ross Wightman and Mitchell Wortsman and Gabriel Ilharco and Cade Gordon and Christoph Schuhmann and Ludwig Schmidt and Jenia Jitsev},
      year={2022},
      eprint={2212.07143},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{schuhmann2022laion5b,
      title={LAION-5B: An open large-scale dataset for training next generation image-text models}, 
      author={Christoph Schuhmann and Romain Beaumont and Richard Vencu and Cade Gordon and Ross Wightman and Mehdi Cherti and Theo Coombes and Aarush Katta and Clayton Mullis and Mitchell Wortsman and Patrick Schramowski and Srivatsa Kundurthy and Katherine Crowson and Ludwig Schmidt and Robert Kaczmarczyk and Jenia Jitsev},
      year={2022},
      eprint={2210.08402},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{gadre2023datacomp,
      title={DataComp: In search of the next generation of multimodal datasets}, 
      author={Samir Yitzhak Gadre and Gabriel Ilharco and Alex Fang and Jonathan Hayase and Georgios Smyrnis and Thao Nguyen and Ryan Marten and Mitchell Wortsman and Dhruba Ghosh and Jieyu Zhang and Eyal Orgad and Rahim Entezari and Giannis Daras and Sarah Pratt and Vivek Ramanujan and Yonatan Bitton and Kalyani Marathe and Stephen Mussmann and Richard Vencu and Mehdi Cherti and Ranjay Krishna and Pang Wei Koh and Olga Saukh and Alexander Ratner and Shuran Song and Hannaneh Hajishirzi and Ali Farhadi and Romain Beaumont and Sewoong Oh and Alex Dimakis and Jenia Jitsev and Yair Carmon and Vaishaal Shankar and Ludwig Schmidt},
      year={2023},
      eprint={2304.14108},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{koh2020understanding,
      title={Understanding Black-box Predictions via Influence Functions}, 
      author={Pang Wei Koh and Percy Liang},
      year={2020},
      eprint={1703.04730},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{basu2021influence,
      title={Influence Functions in Deep Learning Are Fragile}, 
      author={Samyadeep Basu and Philip Pope and Soheil Feizi},
      year={2021},
      eprint={2006.14651},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{pruthi2020estimating,
      title={Estimating Training Data Influence by Tracing Gradient Descent}, 
      author={Garima Pruthi and Frederick Liu and Mukund Sundararajan and Satyen Kale},
      year={2020},
      eprint={2002.08484},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{artiststudy2022,
  author = {Manav Mashruwala},
  title = {SD1.5 Artist Study}, 
  year = {2023},
  howpublished = {\url{https://docs.google.com/spreadsheets/d/1SRqJ7F_6yHVSOeCi3U82aA448TqEGrUlRrLLZ51abLg/edit#gid=1609688219}},
  note = {[Online; accessed 28-11-2023]}
}
